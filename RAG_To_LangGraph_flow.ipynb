{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd309d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936410ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9e69e8c",
   "metadata": {},
   "source": [
    "### This is the simple workflow with graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51039b4",
   "metadata": {},
   "source": [
    "# from here phase- 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4307dbb7",
   "metadata": {},
   "source": [
    "<!-- ![image-2.png](attachment:image-2.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6937f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "020ab978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(input1):\n",
    "    return input1 + \"from first function\"\n",
    "\n",
    "def function2(input2):\n",
    "    return input2 + \" and nikhil from second function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30d37c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show this later on\n",
    "def function3(input3):\n",
    "    return input3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "961d4e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show this later on\n",
    "def function1(input1):\n",
    "    return input1 + \" from first function\"\n",
    "\n",
    "def function2(input2):\n",
    "    output=function3(\"this is function 2 in between\")\n",
    "    return input2 + \" \" + output + \" and ketan from second function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65e77fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nikhil from first function'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function1(\"nikhil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c27c0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mogre this is function 2 in between and ketan from second function'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function2(\"mogre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7146702d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nikhil from first function mogre this is function 2 in between and ketan from second function'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function1(\"nikhil\")+ \" \"+function2(\"mogre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d89d5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8920da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is called a simple graph\n",
    "#we have one more type of graph that is called stategraph\n",
    "workflow1=Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5b1e1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.graph.Graph at 0x11675971940>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow1.add_node(\"function1\", function1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "975c1e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.graph.Graph at 0x11675971940>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow1.add_node(\"function2\",function2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30ee5589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.graph.Graph at 0x11675971940>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow1.add_edge(\"function1\",\"function2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d86096c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.graph.Graph at 0x11675971940>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow1.set_entry_point(\"function1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df191bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.graph.Graph at 0x11675971940>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow1.set_finish_point(\"function2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd89c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "app1=workflow1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9208f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app1.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4874b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi this is nikhil  from first function this is function 2 in between and ketan from second function'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app1.invoke(\"hi this is nikhil \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e6e32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"hi this is nikhil \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc3efe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is output from function1\n",
      "_______\n",
      "hi this is nikhil  from first function\n",
      "\n",
      "\n",
      "here is output from function2\n",
      "_______\n",
      "hi this is nikhil  from first function this is function 2 in between and ketan from second function\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app1.stream(input):\n",
    "    for key,value in output.items():\n",
    "        print(f\"here is output from {key}\")\n",
    "        print(\"_______\")\n",
    "        print(value)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e02348",
   "metadata": {},
   "source": [
    "### now explain the intermediate function\n",
    "\n",
    "#### then come to the next workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d789c",
   "metadata": {},
   "source": [
    "# from here phase-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebabbb7",
   "metadata": {},
   "source": [
    "### Now let's create a workflow with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f619ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "952bc176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f05f4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"]= os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f40e8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model_name=\"qwen-qwq-32b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd7207d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\\n<think>\\nOkay, the user said \"hi\". I should respond in a friendly way. Let me think of a simple greeting. Maybe \"Hello! How can I assist you today?\" That\\'s good. It\\'s welcoming and opens the door for them to ask for help. I\\'ll go with that.\\n</think>\\n\\nHello! How can I assist you today?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 71, 'prompt_tokens': 11, 'total_tokens': 82, 'completion_time': 0.172598529, 'prompt_time': 0.002955874, 'queue_time': 0.242687875, 'total_time': 0.175554403}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_fbb7e6cc39', 'finish_reason': 'stop', 'logprobs': None}, id='run-07eb9ac1-b9c9-4d4e-998b-c919a9028ee6-0', usage_metadata={'input_tokens': 11, 'output_tokens': 71, 'total_tokens': 82})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213844cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eeec1380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Github_All_files\\LangGraph\\LangGraph\\langgraph_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Github_All_files\\LangGraph\\LangGraph\\langgraph_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--BAAI--bge-base-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8ebc396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(input):\n",
    "    llm=ChatGroq(model_name=\"qwen-qwq-32b\")\n",
    "    response=llm.invoke(input).content\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac0e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2(input):\n",
    "    upper_case=input.upper()\n",
    "    return upper_case\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888deb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph\n",
    "workflow2=Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe43cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.graph.Graph at 0x1ca8e2f7ee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow2.add_node(\"llm\",function1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca1939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.graph.Graph at 0x1ca8e2f7ee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow2.add_node(\"upper_string\",function2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215ce1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.graph.Graph at 0x1ca8e2f7ee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow2.add_edge(\"llm\",\"upper_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35f0435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.graph.Graph at 0x1ca8e2f7ee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow2.set_entry_point(\"llm\")\n",
    "workflow2.set_finish_point(\"upper_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61af9ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "app2=workflow2.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e4f802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app2.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991fd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE FIRST PRIME MINISTER OF INDIA WAS JAWAHARLAL NEHRU.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app2.invoke(\"what is a name of first indian prime minister?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0efb061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is output from llm\n",
      "_______\n",
      "Hi Sunny! How can I assist you today?\n",
      "\n",
      "\n",
      "here is output from upper_string\n",
      "_______\n",
      "HI SUNNY! HOW CAN I ASSIST YOU TODAY?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app2.stream(input):\n",
    "    for key,value in output.items():\n",
    "        print(f\"here is output from {key}\")\n",
    "        print(\"_______\")\n",
    "        print(value)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b31bdf7",
   "metadata": {},
   "source": [
    "### this is a second tool which you will create in the end of this phase\n",
    "#### Now lets create our output token counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function3(input):\n",
    "    token=input.split()\n",
    "    token_number=len(token)\n",
    "    token_number=f\"total token number is {token_number}\"\n",
    "    return token_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow3=Graph()\n",
    "workflow3.add_node(\"llm\",function1)\n",
    "workflow3.add_node(\"token_counter\",function3)\n",
    "workflow3.add_edge(\"llm\",\"token_counter\")\n",
    "workflow3.set_entry_point(\"llm\")\n",
    "workflow3.set_finish_point(\"token_counter\")\n",
    "app3=workflow3.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96cbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app3.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c2709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'total token number is 9'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "app3.invoke(\"what is a name of first indian prime minister?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf487f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"ask any question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574603da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is output from llm\n",
      "_______\n",
      "Hi Sunny! How can I assist you today?\n",
      "\n",
      "\n",
      "here is output from token_counter\n",
      "_______\n",
      "8\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app3.stream(input):\n",
    "    for key,value in output.items():\n",
    "        print(f\"here is output from {key}\")\n",
    "        print(\"_______\")\n",
    "        print(value)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e54ea",
   "metadata": {},
   "source": [
    "# from here phase-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6eba9",
   "metadata": {},
   "source": [
    "### Question: Can we integrating a RAG Pipeline?\n",
    "\n",
    "### Answer: yes we can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200cb644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9124f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_17712\\779633069.py:17: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data\\\\llama3.txt'}\n",
      "by Meta AI starting in February 2023.[2][3] The latest version is Llama 3 released in April\n",
      "page_content='by Meta AI starting in February 2023.[2][3] The latest version is Llama 3 released in April' metadata={'source': 'data\\\\llama3.txt'}\n",
      "page_content='The latest version is Llama 3 released in April 2024.[4]' metadata={'source': 'data\\\\llama3.txt'}\n",
      "page_content='art models such as PaLM and Chinchilla.[2]. Meta AI's testing shows that Llama 3 70B beats Gemini,' metadata={'source': 'data\\\\llama3.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "loader=DirectoryLoader(\"./data\",glob=\"./*.txt\",loader_cls=TextLoader)\n",
    "docs=loader.load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "new_docs = text_splitter.split_documents(documents=docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]\n",
    "\n",
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "query = \"what is meta llama3?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c03e29bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caf3ae83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x0000021086399400>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "498cac79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'data\\\\llama3.txt'}\n",
      "by Meta AI starting in February 2023.[2][3] The latest version is Llama 3 released in April\n"
     ]
    }
   ],
   "source": [
    "query = \"what is meta llama3?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d744cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='by Meta AI starting in February 2023.[2][3] The latest version is Llama 3 released in April' metadata={'source': 'data\\\\llama3.txt'}\n",
      "page_content='by Meta AI starting in February 2023.[2][3] The latest version is Llama 3 released in April' metadata={'source': 'data\\\\llama3.txt'}\n",
      "page_content='The latest version is Llama 3 released in April 2024.[4]' metadata={'source': 'data\\\\llama3.txt'}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c0341d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d81b8654",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "798ac232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b9f522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict[\"messages\"]=\"hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dd409c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': 'hi, how are you?'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2da1c148",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict[\"messages\"]=\"hi, what are you doing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dabb94fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': 'hi, what are you doing?'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dca25c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentState={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d60582e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentState[\"messages\"]=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b1e160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentState[\"messages\"].append(\"hi, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1242b6ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['hi, how are you?']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90d2a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentState[\"messages\"].append(\"hi, what you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "30bf700c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['hi, how are you?', 'hi, what you doing?']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2146486d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi, what you doing?'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AgentState[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a2a57c",
   "metadata": {},
   "source": [
    "## what is a work of agent state? \n",
    "### the work of AgentState is just pass the message to the next node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39dc99e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(AgentState):\n",
    "    \n",
    "    message=AgentState[\"messages\"]\n",
    "    \n",
    "    question=message[-1]\n",
    "    \n",
    "    complete_prompt=\"Your task is to provide only the brief answer based on the user query. \\\n",
    "        Don't include too much reasoning. Following is the user query: \" + question\n",
    "    \n",
    "    response = llm.invoke(complete_prompt)\n",
    "    \n",
    "    AgentState['messages'].append(response.content) # appending LLM call response to the AgentState\n",
    "    \n",
    "    #print(AgentState)\n",
    "    \n",
    "    return AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e255cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(AgentState):\n",
    "    messages = AgentState['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "120d432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2231af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Langchain graph\n",
    "workflow4 = Graph()\n",
    "workflow4.add_node(\"LLM\", function_1)\n",
    "workflow4.add_node(\"RAGtool\", function_2)\n",
    "workflow4.add_edge('LLM', 'RAGtool')\n",
    "workflow4.set_entry_point(\"LLM\")\n",
    "workflow4.set_finish_point(\"RAGtool\")\n",
    "app4 = workflow4.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e9f6796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='mermaid.ink', port=443): Read timed out. (read timeout=10)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app4.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da0accfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [\"Tell me about llama3 model\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da3f713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [\"Tell me about llama3 model efficiency\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b381eafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<think>\\nOkay, so I need to answer the question about Llama3 model efficiency based on the provided context. Let me look at the context again to make sure I don\\'t miss anything. The user provided three documents, but they seem to have some repetition. Let me parse each one:\\n\\nFirst document: \"the 8B parameter version of Llama 3 as being \\'surprisingly capable\\' given it\\'s size.[11]\". Second one is the same as the first. Third one says, \"art models such as PaLM and Chinchilla.[2]. Meta AI\\'s testing shows that Llama 3 70B beats Gemini,\". Wait, the third one might have a typo, maybe \"art\" was supposed to be \"compared to\"? Because otherwise, \"art models\" doesn\\'t make much sense in this context. But I\\'ll take it as is unless there\\'s a reason to think otherwise.\\n\\nThe question is about efficiency. The term efficiency in models can relate to parameter count versus performance. So, the first part mentions the 8B version is \"surprisingly capable\" given its size. That suggests that for its parameter count (8 billion), it performs better than expected, which points to efficiency. The 70B version beating Gemini might indicate that even at a larger size, it\\'s efficient in some tasks compared to competitors. The mention of PaLM and Chinchilla in the third document could be for comparison. PaLM and Chinchilla are other large models; Chinchilla from Google is a 73B parameter model, and PaLM is a larger one. If Llama3 70B is beating Gemini (another big model, maybe from Google too?), that could imply that in terms of performance per parameter or in specific tasks, it\\'s efficient.\\n\\nWait, but the exact wording in the third document fragment is a bit unclear. Let me check again: \"art models such as PaLM and Chinchilla.[2]. Meta AI\\'s testing shows that Llama 3 70B beats Gemini,\". Maybe it\\'s supposed to say \"compared to art models\" or \"when compared to PaLM and Chinchilla\". The mention of [2] is a citation, so maybe the original text was comparing Llama3 to those models. Since efficiency can be about how well a model performs relative to its size compared to others, that might be relevant.\\n\\nPutting this together, the key points from the context about efficiency are:\\n\\n1. The 8B model is \"surprisingly capable\" for its size, which suggests that it\\'s efficient because it achieves good performance without a huge number of parameters.\\n2. The 70B model outperforms Gemini, which might indicate that at larger scales, Llama3 is more efficient in terms of performance per parameter or in some specific benchmarks. \\n\\nHowever, the exact details of what efficiency aspect (like training compute, inference speed, etc.) aren\\'t specified here. The user\\'s question is about efficiency, and the context doesn\\'t explicitly mention efficiency metrics like FLOPs or latency, but the capability relative to size implies parameter efficiency. \\n\\nSo the answer should focus on the parameter efficiency of the 8B version being notable, and the 70B version\\'s performance compared to other large models (like Gemini) possibly indicating efficiency in another sense. The citations [11] and [2] are referenced, but since I can\\'t look them up, I should just reference the context given. \\n\\nI need to make sure not to introduce any external knowledge beyond the provided documents. So the answer should mention that the 8B model is surprisingly capable given its size, hence efficient, and that the 70B model outperforms Gemini, which could relate to efficiency in terms of performance relative to other large models. The comparison to PaLM and Chinchilla might also be part of that, so maybe Llama3 is more efficient than those models in some aspects. \\n\\nWait, the third document fragment says \"art models such as PaLM and Chinchilla\" but perhaps that\\'s a typo. Alternatively, maybe it\\'s supposed to say \"compared to art models\", but even if it\\'s \"art\", maybe it\\'s a different category? But given the context, perhaps it\\'s a mistake and it\\'s supposed to be \"other models\". Alternatively, maybe \"art\" is a typo for \"arbitrary\" or \"alternative\"? Since the sentence structure is a bit broken, maybe the original sentence was \"Meta AI\\'s testing shows that Llama 3 70B beats Gemini when compared to models such as PaLM and Chinchilla,\". Assuming that, then the 70B model is efficient in performance compared to those models, perhaps with fewer parameters or better performance per parameter. \\n\\nIn any case, the key points from the given context are the 8B model\\'s capability relative to its size, and the 70B outperforming Gemini. So the answer should highlight those two points as indicating efficiency, especially the 8B\\'s efficiency in being capable despite being smaller, and the 70B\\'s superior performance compared to another large model, suggesting efficiency in resource usage or better optimization.\\n</think>\\n\\nThe context indicates that the Llama 3 model demonstrates notable efficiency in two key aspects:  \\n1. **Parameter Efficiency of the 8B Version**: The 8B parameter variant of Llama 3 is described as \"surprisingly capable\" given its relatively smaller size (8 billion parameters). This suggests that it achieves strong performance without requiring an extremely large parameter count, indicating efficient use of parameters compared to its size.  \\n2. **Competitive Performance of the 70B Version**: The 70B parameter version of Llama 3 outperforms Gemini (another large model, likely from Google), implying that it achieves superior results compared to competitors, possibly with comparable or fewer resources.  \\n\\nThe context also indirectly references comparisons to models like PaLM and Chinchilla (which are known for their size and performance), further suggesting that Llama 3s efficiency stems from balancing parameter count and effectiveness.  \\n\\n(All information is derived directly from the provided excerpts, which highlight capability relative to model size and competitive benchmarks.)'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app4.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9788bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"message\": [\"can you give me 3 property of llama3 model\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dde0d345",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mapp4\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# stream() yields dictionaries with output keyed by node name\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOutput from node \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github_All_files\\LangGraph\\LangGraph\\langgraph_env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2356\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2350\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2351\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2352\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2353\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2354\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2355\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2356\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2357\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2359\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2361\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2362\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2363\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2364\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github_All_files\\LangGraph\\LangGraph\\langgraph_env\\Lib\\site-packages\\langgraph\\pregel\\runner.py:158\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[39m\n\u001b[32m    156\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github_All_files\\LangGraph\\LangGraph\\langgraph_env\\Lib\\site-packages\\langgraph\\pregel\\retry.py:39\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     37\u001b[39m     task.writes.clear()\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     41\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github_All_files\\LangGraph\\LangGraph\\langgraph_env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:622\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m622\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    624\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Github_All_files\\LangGraph\\LangGraph\\langgraph_env\\Lib\\site-packages\\langgraph\\utils\\runnable.py:376\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    374\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    378\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mfunction_1\u001b[39m\u001b[34m(AgentState)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction_1\u001b[39m(AgentState):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     message=\u001b[43mAgentState\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      5\u001b[39m     question=message[-\u001b[32m1\u001b[39m]\n\u001b[32m      7\u001b[39m     complete_prompt=\u001b[33m\"\u001b[39m\u001b[33mYour task is to provide only the brief answer based on the user query. \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33m        Don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt include too much reasoning. Following is the user query: \u001b[39m\u001b[33m\"\u001b[39m + question\n",
      "\u001b[31mKeyError\u001b[39m: 'messages'",
      "During task with name 'LLM' and id '29d78673-3985-4e82-96eb-1c5c60088075'"
     ]
    }
   ],
   "source": [
    "for output in app4.stream(inputs):\n",
    "    # stream() yields dictionaries with output keyed by node name\n",
    "    for key, value in output.items():\n",
    "        print(f\"Output from node '{key}':\")\n",
    "        print(\"---\")\n",
    "        print(value)\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1fe72d",
   "metadata": {},
   "source": [
    "# from here phase-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6359c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d1d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "operator.add(10, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d4601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "operator.sub(10, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0be289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    # The 'messages' field should be a sequence of strings, and we annotate it with 'operator.add'\n",
    "    # This implies we might want to \"add\" new messages to the sequence later\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621459ce",
   "metadata": {},
   "source": [
    "1. TypedDict:\n",
    "- TypedDict is a special type that allows you to define a dictionary in Python where each key has a specific type.\n",
    "Its useful when you want to enforce type checks on the keys and values of a dictionary.\n",
    "\n",
    "2. Annotated:\n",
    "- Annotated is used to add metadata or constraints to a type.\n",
    "Its typically used for adding additional information, like constraints or descriptions, to the type.\n",
    "\n",
    "3. Sequence[BaseMessage]:\n",
    "- Sequence is a generic type from Python's typing module that represents a list-like structure (such as a list or tuple) where the order matters, and you can access elements by their position.\n",
    "\n",
    "4. BaseMessage:\n",
    "- Basemessage: This assumes you're working with the LangChain framework, and BaseMessage is part of its core messaging system. It is typically used when handling or processing messages in LangChain's workflows\n",
    "\n",
    "5. operator.add:\n",
    "\n",
    "- operator.add is a function from Pythons operator module that performs addition (+) on its arguments.\n",
    "In this context, operator.add is being used as metadata for the Annotated type, likely suggesting that the sequence of BaseMessage objects is meant to be \"added\" or concatenated with another sequence at some point. Its an indicator of how the messages field will be treated in the logic of your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae4bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class Person(TypedDict):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "# Correct usage\n",
    "person: Person = {\n",
    "    \"name\": \"Alice\",\n",
    "    \"age\": 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7863efbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "person[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e6b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "person[\"age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7699de6",
   "metadata": {},
   "source": [
    "### Putting It All Together:\n",
    "\n",
    "- The line messages: Annotated[Sequence[BaseMessage], operator.add] defines a field messages that:\n",
    "\n",
    "- Must be a sequence (like a list or tuple) of BaseMessage objects.\n",
    "Has the additional annotation operator.add, which suggests that this sequence is intended to support addition (i.e., concatenation of multiple sequences of messages).\n",
    "\n",
    "messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "\"messages\": [BaseMessage(\"Hello!\", \"Alice\"), BaseMessage(\"Hi!\", \"Bob\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96745679",
   "metadata": {},
   "source": [
    "**Pydantic is a Python library for data validation and settings management using Python-type annotations. It ensures that the data you work with matches your specified data types, simplifying error handling and data parsing in Python applications.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983355c8",
   "metadata": {},
   "source": [
    "https://docs.pydantic.dev/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e0e1a",
   "metadata": {},
   "source": [
    "1. BaseModel (from Pydantic)\n",
    "- BaseModel is the base class from which all Pydantic models inherit.\n",
    "- It provides built-in validation, serialization, and parsing of data.\n",
    "- When you create a class that inherits from BaseModel, you are essentially defining a data model with strict type checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f80d4a",
   "metadata": {},
   "source": [
    "##### The BaseModel class is used for data validation and settings management in Pydantic. It's the core class that helps define data models, ensuring that attributes follow specific types and validation rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1c1011",
   "metadata": {},
   "source": [
    "2. Field\n",
    "The Field function is used to provide additional metadata for each field in the model. You can use it to:\n",
    "\n",
    "- Add descriptions (as seen here).\n",
    "- Set default values.\n",
    "- Add validators (e.g., minimum length).\n",
    "- Add constraints (like making a field optional or required)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f87b6",
   "metadata": {},
   "source": [
    "#### Important Classes and Functions:\n",
    "- BaseModel: The core class for defining data models.\n",
    "- Field: Provides additional metadata and validation for model fields.\n",
    "- ValidationError: Raised when validation fails.\n",
    "- Constrained Types: constr, conint, conlist, etc., for restricting field values.\n",
    "- Predefined Types: EmailStr, AnyUrl, HttpUrl, etc., for common validations.\n",
    "- RootValidator: Used for validating the entire model or handling field interdependencies.\n",
    "- Config: Model configuration options.\n",
    "- parse_obj, parse_raw: Functions to parse data into models.\n",
    "- validate_arguments: A decorator for function argument validation.\n",
    "- Strict Types: Enforces exact type validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a002cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pydantic import BaseModel,Filed,ValidationError,EmailStr,root_validator,validate_arguments,StrictInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# Define a Pydantic model\n",
    "class User(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    email: str\n",
    "\n",
    "# Create an instance of the model with valid data\n",
    "user = User(id=1, name=\"Alice\", email=\"alice@example.com\")\n",
    "\n",
    "# Access the attributes\n",
    "print(user.name)  # Output: Alice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244cfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 1 validation error for User\n",
      "id\n",
      "  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='abc', input_type=str]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/int_parsing\n"
     ]
    }
   ],
   "source": [
    "# Automatic validation for types\n",
    "try:\n",
    "    invalid_user = User(id=\"abc\", name=\"Bob\", email=\"bob@example.com\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7e7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic: str = Field(description='Selected Topic')\n",
    "    Reasoning: str = Field(description='Reasoning behind topic selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48200e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic: str = Field(description='Selected Topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35668b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FieldInfo(annotation=NoneType, required=True, description='Selected Topic')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d00771",
   "metadata": {},
   "source": [
    "##### just to check the pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb1ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Intelligence\n",
      "It's a rapidly growing field with many advancements.\n"
     ]
    }
   ],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "# Valid data\n",
    "data = {\n",
    "    \"Topic\": \"Artificial Intelligence\",\n",
    "    \"Reasoning\": \"It's a rapidly growing field with many advancements.\"\n",
    "}\n",
    "\n",
    "# Parsing and validating the data using the Pydantic model\n",
    "try:\n",
    "    topic_selection = TopicSelectionParser(**data)\n",
    "    print(topic_selection.Topic)  # Output: Artificial Intelligence\n",
    "    print(topic_selection.Reasoning)  # Output: It's a rapidly growing field with many advancements.\n",
    "except ValidationError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f1b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 validation error for TopicSelectionParser\n",
      "Reasoning\n",
      "  Input should be a valid string [type=string_type, input_value=12345, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.9/v/string_type\n"
     ]
    }
   ],
   "source": [
    "# Invalid data (Reasoning is not a string)\n",
    "invalid_data = {\n",
    "    \"Topic\": \"Artificial Intelligence\",\n",
    "    \"Reasoning\": 12345  # Invalid type, should be a string\n",
    "}\n",
    "\n",
    "# This will raise a validation error\n",
    "try:\n",
    "    topic_selection = TopicSelectionParser(**invalid_data)\n",
    "    print(topic_selection.Topic)  # Output: Artificial Intelligence\n",
    "    print(topic_selection.Reasoning)  # Output: It's a rapidly growing field with many advancements.\n",
    "except ValidationError as e:\n",
    "    print(e)  # This will print a detailed error message about the invalid field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0570b",
   "metadata": {},
   "source": [
    "- **TopicSelectionParser:** A Pydantic model that defines two fields (Topic and Reasoning), both of which are required to be strings.\n",
    "- **Field:** Adds metadata, like descriptions, and can also be used for validation and constraints.\n",
    "- **Pydantics Role:** Ensures that the data matches the expected structure and types, raising validation errors if the data is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1dc655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b545af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PydanticOutputParser(pydantic_object=<class '__main__.TopicSelectionParser'>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb00bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PydanticOutputParser.parse of PydanticOutputParser(pydantic_object=<class '__main__.TopicSelectionParser'>)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parser.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4089b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"Topic\": {\"description\": \"Selected Topic\", \"title\": \"Topic\", \"type\": \"string\"}, \"Reasoning\": {\"description\": \"Reasoning behind topic selection\", \"title\": \"Reasoning\", \"type\": \"string\"}}, \"required\": [\"Topic\", \"Reasoning\"]}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5d3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
    "\n",
    "the object {\"foo\": [\"bar\", \"baz\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3166ded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic='Advancements in AI' Reasoning='AI has significantly improved in recent years due to advances in deep learning.'\n"
     ]
    }
   ],
   "source": [
    "# Example prompt template for topic selection\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"user_input\"],\n",
    "    template=\"Based on the input: {user_input}, please select a topic and provide reasoning.\"\n",
    ")\n",
    "\n",
    "# Simulate generating an output (in a real scenario, this would be from an LLM)\n",
    "user_input = \"Discuss advancements in AI.\"\n",
    "output = {\n",
    "    \"Topic\": \"Advancements in AI\",\n",
    "    \"Reasoning\": \"AI has significantly improved in recent years due to advances in deep learning.\"\n",
    "}\n",
    "\n",
    "# Parse the output using the PydanticOutputParser\n",
    "try:\n",
    "    # Instead of directly parsing, create an instance of TopicSelectionParser\n",
    "    parsed_result = TopicSelectionParser(**output)\n",
    "    # Alternatively, if you prefer to use PydanticOutputParser:\n",
    "    # parsed_result = parser.parse(output)  # This will not work since output is a dict, not a string\n",
    "\n",
    "    # Display the parsed result\n",
    "    print(parsed_result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17737619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '..\\\\data2\\\\japan.txt'}\n",
      "Industrial revival hope for Japan\n",
      "page_content='Industrial revival hope for Japan' metadata={'source': '..\\\\data2\\\\japan.txt'}\n",
      "page_content='Industrial revival hope for Japan' metadata={'source': '..\\\\data2\\\\japan.txt'}\n",
      "page_content='Industrial revival hope for Japan' metadata={'source': '..\\\\data2\\\\japan.txt'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "loader2=DirectoryLoader(\"../data2\",glob=\"./*.txt\",loader_cls=TextLoader)\n",
    "docs2=loader2.load()\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "new_docs2 = text_splitter.split_documents(documents=docs2)\n",
    "doc_strings2 = [doc.page_content for doc in new_docs2]\n",
    "\n",
    "db2 = Chroma.from_documents(new_docs2, embeddings)\n",
    "retriever2 = db2.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "query = \"Tell me about Japan's Industrial Growth?\"\n",
    "docs = retriever2.get_relevant_documents(query)\n",
    "print(docs[0].metadata)\n",
    "print(docs[0].page_content)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef87b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "class TopicSelectionParser(BaseModel):\n",
    "    Topic: str = Field(description='Selected Topic')\n",
    "    Reasoning: str = Field(description='Reasoning behind topic selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c77fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state):\n",
    "    message=state[\"messages\"]\n",
    "    question=message[-1]\n",
    "    print(question)\n",
    "    \n",
    "    template=\"\"\"\n",
    "    Your task is to classify the given user query into one of the following categories: [Japan, Sports, Not Related]. \n",
    "    Only respond with the category name and nothing else.\n",
    "\n",
    "    User query: {question}\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=template,\n",
    "                                    input_variables=[question],\n",
    "                                    partial_variables={\n",
    "                                        \"format_instructions\" : parser.get_format_instructions()                                    }\n",
    "                                    )\n",
    "    chain =  prompt | llm | parser\n",
    "    \n",
    "    response = chain.invoke({\"question\":question,\"format_instructions\" : parser.get_format_instructions() })\n",
    "\n",
    "    print(response)\n",
    "\n",
    "    return {\"messages\": [response.Topic]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37c4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "state={\"messages\": [\"Tell me about Japan's Industrial Growth\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34974fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about Japan's Industrial Growth\n",
      "Topic='Japan' Reasoning=\"The user query specifically asks about Japan's Industrial Growth, which falls under the category of information about Japan.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': ['Japan']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function_1(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f98291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(state):\n",
    "    print('-> Calling RAG ->')\n",
    "    messages = state['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    retrieval_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "    result = retrieval_chain.invoke(question)\n",
    "    return  {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afe3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_3(state):\n",
    "    print('-> Calling LLM ->')\n",
    "\n",
    "    messages = state['messages']\n",
    "    question = messages[0] ## Fetching the user question\n",
    "\n",
    "    # Normal LLM call\n",
    "    complete_query = \"Anwer the follow question with you knowledge of the real world. Following is the user question: \" + question\n",
    "    response = llm.invoke(complete_query)\n",
    "    return {\"messages\": [response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929879e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state):\n",
    "    print('-> Router ->')\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    print(last_message)\n",
    "    if 'Japan' in last_message or 'Sports' in last_message:\n",
    "        return 'RAG Call'\n",
    "    else:\n",
    "        return 'LLM Call'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,END\n",
    "\n",
    "workflow5 = StateGraph(AgentState) ### StateGraph with AgentState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0406db1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x298e4ec5ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow5.add_node(\"agent\", function_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a373538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x298e4ec5ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow5.add_node(\"RAG\", function_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d7d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x298e4ec5ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow5.add_node(\"LLM\", function_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152d44b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x298e4ec5ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow5.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1876f9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x298e4ec5ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow5.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    router,\n",
    "    {\n",
    "        \"RAG Call\": \"RAG\",\n",
    "        \"LLM Call\": \"LLM\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x298e4ec5ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow5.add_edge(\"RAG\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f89e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x298e4ec5ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow5.add_edge(\"LLM\",END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app5=workflow5.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebbf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app5.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [\"Tell me about Japan's Industrial Growth\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6288306b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic='Japan' Reasoning=\"The user query specifically asks about Japan's Industrial Growth, which is related to the category 'Japan'.\"\n",
      "-> Router ->\n",
      "Japan\n",
      "-> Calling RAG ->\n"
     ]
    }
   ],
   "source": [
    "output = app5.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc8da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [\"Tell me about Japan's Industrial Growth\",\n",
       "  'Japan',\n",
       "  'Based on the provided context, it appears that there is an \"Industrial revival hope for Japan.\" This information is repeated across multiple documents, suggesting that there is a significant focus on the potential for industrial growth in Japan.']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [\"Tell me about first prime minister of india?\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6940dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic='Not Related' Reasoning='The query is about the first prime minister of India, which is not related to Japan or sports.'\n",
      "-> Router ->\n",
      "Not Related\n",
      "-> Calling LLM ->\n"
     ]
    }
   ],
   "source": [
    "output = app5.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704338e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['Tell me about first prime minister of india?',\n",
       "  'Not Related',\n",
       "  'The first Prime Minister of India was Jawaharlal Nehru. He served as the Prime Minister from August 15, 1947, until his death on May 27, 1964. Nehru was a key figure in the Indian independence movement and played a significant role in drafting the Constitution of India. He was known for his vision of a secular, socialist, and democratic India and his commitment to economic development and social justice.']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3122df",
   "metadata": {},
   "source": [
    "# from here phase-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac516107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e41309",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(first_number: int, second_number: int):\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return first_number * second_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c7eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[convert_to_openai_tool(multiply)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = llm.bind(tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05df34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model_with_tools.invoke('What is 35 * 46?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a913c5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_036z', 'function': {'arguments': '{\"first_number\": 35, \"second_number\": 46}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 209, 'total_tokens': 242, 'completion_time': 0.102965805, 'prompt_time': 0.016752075, 'queue_time': 0.016044961, 'total_time': 0.11971788}, 'model_name': 'llama3-groq-70b-8192-tool-use-preview', 'system_fingerprint': 'fp_ee4b521143', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-9013b430-16a1-462d-97d2-770edf7bb39a-0', tool_calls=[{'name': 'multiply', 'args': {'first_number': 35, 'second_number': 46}, 'id': 'call_036z', 'type': 'tool_call'}], usage_metadata={'input_tokens': 209, 'output_tokens': 33, 'total_tokens': 242})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls = response.additional_kwargs.get('tool_calls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009ace8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'call_036z',\n",
       "  'function': {'arguments': '{\"first_number\": 35, \"second_number\": 46}',\n",
       "   'name': 'multiply'},\n",
       "  'type': 'function'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc0c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Name: multiply\n",
      "Function Arguments: {\"first_number\": 35, \"second_number\": 46}\n",
      "{'id': 'call_036z', 'function': {'arguments': '{\"first_number\": 35, \"second_number\": 46}', 'name': 'multiply'}, 'type': 'function'}\n"
     ]
    }
   ],
   "source": [
    "for tool_call in tool_calls:\n",
    "    print('Function Name:',tool_call.get('function').get('name'))\n",
    "    print('Function Arguments:',tool_call.get('function').get('arguments'))\n",
    "    print(tool_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797d5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(state):\n",
    "    messages = state['messages']\n",
    "    question = messages[-1]   ## Fetching the user question\n",
    "    return {\"messages\":[model_with_tools.invoke(question)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e249501a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_tool(state):\n",
    "    tool_calls = state['messages'][-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    print(f\"here is a tool_calls {tool_calls}\")\n",
    "    multiply_call = None\n",
    "\n",
    "    for tool_call in tool_calls:\n",
    "        if tool_call.get(\"function\").get(\"name\") == \"multiply\":\n",
    "            multiply_call = tool_call\n",
    "\n",
    "    if multiply_call is None:\n",
    "        raise Exception(\"No adder input found.\")\n",
    "\n",
    "    res = multiply.invoke(\n",
    "        json.loads(multiply_call.get(\"function\").get(\"arguments\"))\n",
    "    )\n",
    "\n",
    "    return {\"messages\" : [res]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167a19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x298e4bd04c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph,END\n",
    "workflow6 = StateGraph(AgentState) ### StateGraph with AgentState\n",
    "workflow6.add_node(\"agent\", invoke_model)\n",
    "workflow6.add_node(\"tool\", invoke_tool)\n",
    "workflow6.add_edge(\"tool\", END)\n",
    "workflow6.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7dcb5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x298e4bd04c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def router(state):\n",
    "    tool_calls = state['messages'][-1].additional_kwargs.get(\"tool_calls\", [])\n",
    "    if len(tool_calls):\n",
    "        return \"multiply\"\n",
    "    else:\n",
    "        return \"end\"\n",
    "\n",
    "workflow6.add_conditional_edges(\"agent\", router, {\n",
    "    \"multiply\": \"tool\",\n",
    "    \"end\": END,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app6 = workflow6.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61bac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app6.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d3853",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1={\"messages\": [\"What is 123 * 456?\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c10072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is a tool_calls [{'id': 'call_5gwn', 'function': {'arguments': '{\"first_number\": 123, \"second_number\": 456}', 'name': 'multiply'}, 'type': 'function'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = app6.invoke({\"messages\": [\"What is 123 * 456?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed8435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56088"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output['messages'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ca31d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = app6.invoke({\"messages\": [\"What is LLM?\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9051c57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LLM can stand for several things, depending on the context:\\n\\n1. Large Language Model: LLM is a term used in the field of artificial intelligence to describe a type of language model that has been trained on a large amount of data and is capable of understanding and generating human-like language. These models are often used for natural language processing tasks such as text generation, language translation, and question answering.\\n\\n2. Master of Laws: LLM is also an abbreviation for the degree Master of Laws, which is a postgraduate law degree that typically takes one year to complete. It is designed for students who have already earned a law degree and want to specialize in a particular area of law.\\n\\n3. Linear Least Squares Method: LLM is also used in mathematics and statistics to refer to the Linear Least Squares Method, which is a method for finding the best fit line to a set of data points by minimizing the sum of the squares of the residuals.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 206, 'total_tokens': 399, 'completion_time': 0.617797947, 'prompt_time': 0.016157627, 'queue_time': 0.017356722999999998, 'total_time': 0.633955574}, 'model_name': 'llama3-groq-70b-8192-tool-use-preview', 'system_fingerprint': 'fp_ee4b521143', 'finish_reason': 'stop', 'logprobs': None} id='run-6bc9e8ce-42e6-4392-9e5b-de2ee49a7e54-0' usage_metadata={'input_tokens': 206, 'output_tokens': 193, 'total_tokens': 399}\n"
     ]
    }
   ],
   "source": [
    "print(output['messages'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "input={\"messages\": [\"What is LLM?\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e892cfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here is output from agent\n",
      "_______\n",
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_j9xa', 'function': {'arguments': '{\"first_number\": 123, \"second_number\": 456}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 209, 'total_tokens': 242, 'completion_time': 0.103375904, 'prompt_time': 0.016173896, 'queue_time': 0.016782064, 'total_time': 0.1195498}, 'model_name': 'llama3-groq-70b-8192-tool-use-preview', 'system_fingerprint': 'fp_ee4b521143', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f08b17bc-a0c3-4230-9792-0a34922fc330-0', tool_calls=[{'name': 'multiply', 'args': {'first_number': 123, 'second_number': 456}, 'id': 'call_j9xa', 'type': 'tool_call'}], usage_metadata={'input_tokens': 209, 'output_tokens': 33, 'total_tokens': 242})]}\n",
      "\n",
      "\n",
      "here is a tool_calls [{'id': 'call_j9xa', 'function': {'arguments': '{\"first_number\": 123, \"second_number\": 456}', 'name': 'multiply'}, 'type': 'function'}]\n",
      "here is output from tool\n",
      "_______\n",
      "{'messages': [56088]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for output in app6.stream(input1):\n",
    "    for key,value in output.items():\n",
    "        print(f\"here is output from {key}\")\n",
    "        print(\"_______\")\n",
    "        print(value)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4956a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14807643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\G'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\G'\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_13964\\1096852281.py:7: SyntaxWarning: invalid escape sequence '\\G'\n",
      "  loader = DirectoryLoader('D:\\Github_All_files\\LangGraph\\LangGraph\\data', glob=\"./*.txt\", loader_cls=TextLoader)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "### Reading the txt files from source directory\n",
    "\n",
    "loader = DirectoryLoader('D:\\Github_All_files\\LangGraph\\LangGraph\\data', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "### Creating Chunks using RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "new_docs = text_splitter.split_documents(documents=docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0a53f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd1703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
